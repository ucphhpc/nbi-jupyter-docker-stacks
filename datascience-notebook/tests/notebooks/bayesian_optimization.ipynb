{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://github.com/bayesian-optimization/BayesianOptimization/blob/master/examples/basic-tour.ipynb\n",
    "\n",
    "## 1. Specifying the function to be optimized\n",
    "\n",
    "This is a function optimization package, therefore the first and most important ingreedient is, of course, the function to be optimized.\n",
    "\n",
    "**DISCLAIMER:** We know exactly how the output of the function below depends on its parameter. Obviously this is just an example, and you shouldn't expect to know it in a real scenario. However, it should be clear that you don't need to. All you need in order to use this package (and more generally, this technique) is a function `f` that takes a known set of parameters and outputs a real number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_box_function(x, y):\n",
    "    \"\"\"Function with unknown internals we wish to maximize.\n",
    "\n",
    "    This is just serving as an example, for all intents and\n",
    "    purposes think of the internals of this function, i.e.: the process\n",
    "    which generates its output values, as unknown.\n",
    "    \"\"\"\n",
    "    return -(x**2) - (y - 1) ** 2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting Started\n",
    "\n",
    "All we need to get started is to instanciate a `BayesianOptimization` object specifying a function to be optimized `f`, and its parameters with their corresponding bounds, `pbounds`. This is a constrained optimization technique, so you must specify the minimum and maximum values that can be probed for each parameter in order for it to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounded region of parameter space\n",
    "pbounds = {\"x\": (2, 4), \"y\": (-3, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    f=black_box_function,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BayesianOptimization object will work out of the box without much tuning needed. The main method you should be aware of is `maximize`, which does exactly what you think it does.\n",
    "\n",
    "There are many parameters you can pass to maximize, nonetheless, the most important ones are:\n",
    "- `n_iter`: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n",
    "- `init_points`: How many steps of **random** exploration you want to perform. Random exploration can help by diversifying the exploration space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     x     |     y     |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-7.135   \u001b[0m | \u001b[0m 2.834   \u001b[0m | \u001b[0m 1.322   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-7.78    \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m-1.186   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-19.0    \u001b[0m | \u001b[0m 4.0     \u001b[0m | \u001b[0m 3.0     \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-16.3    \u001b[0m | \u001b[0m 2.378   \u001b[0m | \u001b[0m-2.413   \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m-4.441   \u001b[0m | \u001b[95m 2.105   \u001b[0m | \u001b[95m-0.005822\u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best combination of parameters and target value found can be accessed via the property `bo.max`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target': -4.441293113411222, 'params': {'y': -0.005822117636089974, 'x': 2.104665051994087}}\n"
     ]
    }
   ],
   "source": [
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the list of all parameters probed and their corresponding target values is available via the property `bo.res`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: \n",
      "\t{'target': -7.135455292718879, 'params': {'y': 1.3219469606529488, 'x': 2.8340440094051482}}\n",
      "Iteration 1: \n",
      "\t{'target': -7.779531005607566, 'params': {'y': -1.1860045642089614, 'x': 2.0002287496346898}}\n",
      "Iteration 2: \n",
      "\t{'target': -19.0, 'params': {'y': 3.0, 'x': 4.0}}\n",
      "Iteration 3: \n",
      "\t{'target': -16.29839645063864, 'params': {'y': -2.412527795983739, 'x': 2.3776144540856503}}\n",
      "Iteration 4: \n",
      "\t{'target': -4.441293113411222, 'params': {'y': -0.005822117636089974, 'x': 2.104665051994087}}\n"
     ]
    }
   ],
   "source": [
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Changing bounds\n",
    "\n",
    "During the optimization process you may realize the bounds chosen for some parameters are not adequate. For these situations you can invoke the method `set_bounds` to alter them. You can pass any combination of **existing** parameters and their associated new bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.set_bounds(new_bounds={\"x\": (-2, 3)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     x     |     y     |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-5.145   \u001b[0m | \u001b[0m 2.115   \u001b[0m | \u001b[0m-0.2924  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-5.379   \u001b[0m | \u001b[0m 2.337   \u001b[0m | \u001b[0m 0.04124 \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m-3.581   \u001b[0m | \u001b[95m 1.874   \u001b[0m | \u001b[95m-0.03428 \u001b[0m |\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m-2.624   \u001b[0m | \u001b[95m 1.702   \u001b[0m | \u001b[95m 0.1472  \u001b[0m |\n",
      "| \u001b[95m 10      \u001b[0m | \u001b[95m-1.762   \u001b[0m | \u001b[95m 1.442   \u001b[0m | \u001b[95m 0.1735  \u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "optimizer.maximize(\n",
    "    init_points=0,\n",
    "    n_iter=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Guiding the optimization\n",
    "\n",
    "It is often the case that we have an idea of regions of the parameter space where the maximum of our function might lie. For these situations the `BayesianOptimization` object allows the user to specify specific points to be probed. By default these will be explored lazily (`lazy=True`), meaning these points will be evaluated only the next time you call `maximize`. This probing process happens before the gaussian process takes over.\n",
    "\n",
    "Parameters can be passed as dictionaries such as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.probe(\n",
    "    params={\"x\": 0.5, \"y\": 0.7},\n",
    "    lazy=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or as an iterable. Beware that the order has to be alphabetical. You can usee `optimizer.space.keys` for guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'y']\n"
     ]
    }
   ],
   "source": [
    "print(optimizer.space.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.probe(\n",
    "    params=[-0.3, 0.1],\n",
    "    lazy=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     x     |     y     |\n",
      "-------------------------------------------------\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m 0.66    \u001b[0m | \u001b[95m 0.5     \u001b[0m | \u001b[95m 0.7     \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m-0.3     \u001b[0m | \u001b[0m 0.1     \u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "optimizer.maximize(init_points=0, n_iter=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Saving and loading the optimizer¶\n",
    "The optimizer state can be saved to a file and loaded from a file. This is useful for continuing an optimization from a previous state, or for analyzing the optimization history without running the optimizer again.\n",
    "\n",
    "Note: if you are using your own custom acquisition function, you will need to save and load the acquisition function state as well. This is done by calling the get_acquisition_params and set_acquisition_params methods of the acquisition function. See the acquisition function documentation for more information.\n",
    "\n",
    "4.1 Saving the optimizer state¶\n",
    "The optimizer state can be saved to a file using the save_state method. optimizer.save_state(“./optimizer_state.json”)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
